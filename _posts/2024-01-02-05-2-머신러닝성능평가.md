---
title : '05_2.머신러닝 성능평가' 
# permalink: /KT_DX/04_2.API/
excerpt : "05.머신러닝/05_2.머신러닝 성능평가"
categories : "KT_AIVLE_SCHOOL"
# tag : #"github.io"
date : '2024-01-02'

toc : True
toc_labels: True
toc_sticky : True
---
## 머신러닝 성능평가

머신러닝의 종류에 따라 성능평가의 목적과 지표가 달라진다

분류 : 정확도 높이기
- 정확하게 예측한 비율로 모델 평가

회귀 : 오차 줄이기
- 실제값과 예측값의 차이인 오차로 모델 평가


### 회귀에 대한 성능평가

$ y $ : 
$ \bar y $ : 
$ \hat y $ : 


- 실제값 : 실제로 예측하고 싶은 값
- 평균값 : 아마 알고 있는, 평균으로 예측한 값
    - 평균을 기준으로 더 좋은 모델을 생성해야함 -> 오차를 더 줄여야 한다
- 예측값 : 모델로 예측한 값
    - 예측값의 정확도를 높이는 것이 중요하다     
    -> 예측값과 실제값의 차이가 줄어들어야 한다     
    -> 오차를 줄이는 것이 중요!

### 오차의 종류

오차 : 실제값과 예측값의 차이

- 오차의 합은 0에 가까워지기 때문에 전체의 오차를 명확하게 알 수 없다
- 오차가 작을 수록 좋은 모델
-> 오차의 합을 구하는 다양한 방법이 필요

| 용어 | 설명 |
| --- | --- |
| SSE <br> (Sum Squared Error) | • 오차의 합 <br> • 오차의 단순 합 |
| MSE <br> (Mean SSE) | • 오차의 제곱의 합을 구한 후 평균 |
| RMSE <br> (Root MSE) | • 오차 제곱의 합을 구한 후 평균을 제곱근 한 것 <br> • MSE의 제곱근 |
| MAE <br> (Mean Absolute Error) | • 오차의 절대값 합을 구한 후 평균을 구함 |
| MAPE <br> (Mean Absolute Percentage Error) | • 오차의 절대값 합의 비율 |



| 용어 | 설명 |
| --- | --- |
| SST<br> (Sum Squared Total) | • 전체 오차 <br> • 실제값과 평균값의 오차 <br> • 적어도 평균보다는 좋은 모델을 생성해야 함 -> 우리에게 허용된 오차 |
| SSR <br>(Sum Squared Regression) | • 전체 오차 중 회귀식이 찾아낸 오차 |
| SSE <br> (Sum Squared Error) | • 전체 오차 중 회귀식이 찾아내지 못한 오차 <br> • SSE가 적을 수록 좋은 모델 |

- SST = SSR + SSE


#### R^2 결정 계수 : R-Squared

- 전체 오차 중 회귀식이 설명할 수 있는 오차의 비율
- 오차의 비 or 설명력이라고도 함
- 1에 가까울 수록 모델이 데이터를 잘 학습했다는 뜻

R^2 = SSR / SST = 1-SSE/SST

```python
# 함수 불러오기

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import r2_score

# 평가하기

# mean_absolute_error 실젯값, 예측값

print(mean_absolute_error(y_test , y_pred))

```



### 분류 모델 성능 평가

분류모델 성능 평가

- 예측값이 실제 값과 많이 같을 수록 좋은 모델이라고 할 수 있음

| 용어 | 설명 |
| --- | --- |
| TN(True Negative) : 진음성 | • 음성으로 잘 예측한 것 <br> • 음성을 음성이라고 예측한 것 |
| FN(False Positive) : 위양성 | • 양성으로 잘못 예측한 것 <br> • 음성을 양성이라고 예측한 것 |
| FP(False Negative) : 위음성 | • 음성으로 잘못 예측한 것 <br> • 양성을 음성이라고 예측한 것 |
| TP(True Positive) : 진양성 | • 양성으로 잘 예측한 것 <br> • 양성을 양성이라고 예측한 것 |



| 용어 | 설명 | 수식 |
| --- | --- | --- |
| 정확도 (accuracy) | • 전체 중에 정확히 맞춘 것의 비율 <br> • 정 분류율 <br> • 가장 직관적으로 모델 성능을 확인 | $ TN+TP \over (TN+FP+FN+TP) $ |
| 정밀도 (precision) | • 1이라 예측한 것 중에 실제값도 1인 것의 비율 <br> • 정밀도가 낮을 경우, 비용 발생 <br> • 예측 관점 | $ TP\over (FP+TP) $ |
| 재현율 (recall) | • 실제값이 1인 것을 1이라 예측한 비율 <br> • 민감도 (Sensitivity) 라고 부르는 경우가 많음 <br> • 재현율이 낮을 경우, 심각한 결과 초래 <br> • 실제 관점 | $ TP \over (FN+TP) $ |
| 특이도 (Specificity) | • 실제 0 중에 0으로 예측한 비율 <br> • 특이도가 낮을 경우, 비용 발생 | $ TN \over (TN+FP) $ |

F1-Score

- 정밀도와 재현율의 조화 평균
- 분자가 같지만 분모가 다를 경우 -> 관점이 다른 경우 조화 평균이 큰 의미를 가짐
- 정밀도와 재현율이 적절하게 요구될 때 사용

$$ F1-Score = 2 * precision * recall / (precision + recall) $$ 

- 산술 평균 : $ (a+b)/2 $ 
- 기하 평균 : $ \sqrt{ab} $ 
- 조화 평균 : $ 2ab/(a+b) $ 

```python
# 함수 불러오기

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# 평가하기

# accuracy_score 실젯값, 예측값

accuracy_score(y_test , y_pred)
precision_score(y_test , y_pred , average=None)

# 매개변수로 평균을 표시할지 개별값으로 표시할 지 지정

```

